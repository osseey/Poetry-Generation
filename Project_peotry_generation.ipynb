{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23KpIezdSpnn"
      },
      "source": [
        "<center><img src=\"data:image/png;base64,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\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI4q50sCSpn3"
      },
      "source": [
        "<h1><center><strong><font color=\"chillipepper\">IA 717: Poetry Generation Project</font></strong></center></h1>\n",
        "<h3><center><font color=\"blue\"><strong>Complete Version</strong></font></center></h3>\n",
        "\n",
        "<center>\n",
        "<h3> Project Supervisor <br/> Cyril Chhun</h3>\n",
        "<email>cyril.chhun@telecom-paris.fr</email>\n",
        "<br/>\n",
        "Year 2021-2022\n",
        "</center>\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contributions :** \n",
        "- Wenjie GuoDuthoit\n",
        "- Ossee Yiboe"
      ],
      "metadata": {
        "id": "NttMRwHPYp67"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1amhcU4Vga"
      },
      "source": [
        "\n",
        "# ⚠ IMPORTANT ⚠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rVhPG1iL90U"
      },
      "source": [
        "Start by creating your own copy of this notebook: in the top menu, select `File` and then `Save a copy in Drive` (or the equivalent instructions in French). Make sure that it created a copy in a folder named `Colab Notebooks` at the root of your Google Drive.\n",
        "\n",
        "Also, when you train the model (so, not for now; a reminder will be issued when it is relevant), you should select a GPU runtime for faster training. In the `Runtime` menu, you can do so by selecting `Change runtime type` and choose `GPU`. Google Colab restricts GPU usage so wait until you actually need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tlj2ffC9LKF",
        "outputId": "da3d5e2c-d413-4791-abbf-0a0a3be0ca14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDaLnGFIbG8a"
      },
      "source": [
        "Now, inside the `Colab Notebooks` folder, create two empty folders named `dramacode_corpus` and `dramacode_checkpoints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks4SAM0p9Yf7"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "else:\n",
        "    ROOT_PATH = './'\n",
        "DRAMACODE_PATH = ROOT_PATH + \"dramacode.github.io/\"\n",
        "NAKED_PATH = DRAMACODE_PATH + \"naked/\"\n",
        "CORPUS_PATH = ROOT_PATH + \"dramacode_corpus\"\n",
        "CHECKPOINT_PATH = ROOT_PATH + \"dramacode_checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3wHdtYJL7Cu"
      },
      "source": [
        "# Poetry generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaV8-6rIMIcf"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zZjsJtKMJko"
      },
      "source": [
        "In this project, we will tackle automatic French poetry generation.\n",
        "\n",
        "Since this task is pretty ambitious, we will need rather complex language models. We will be using recurrent neural networks (RNN), and particularly Long-Short Term Memory (LSTM) cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3NNDxk4LLlI"
      },
      "source": [
        "### Task Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VdqpXyfY8xG"
      },
      "source": [
        "Let us begin with defining our task more precisely.\n",
        "\n",
        "In this project, we will focus on verse, specifically [alexandrines](https://www.wikiwand.com/fr/Alexandrin), i.e. twelve-syllable verse.\n",
        "\n",
        "For example, this famous line from Racine's *Phèdre* is one of them:\n",
        "> Tout m'afflige et me nuit, et conspire à me nuire.\n",
        "\n",
        "We will also want these verses to rhyme in pairs, in what we call \"flat\" rhymes, i.e., of the form AABB(CC...).\n",
        "\n",
        "> Tout m'afflige et me nuit, et conspire à me nuire.  \n",
        "> Comme on voit tous ses vœux l’un l’autre se détruire !  \n",
        "> Vous-même, condamnant vos injustes desseins,  \n",
        "> Tantôt à vous parer vous excitiez nos mains ;\n",
        "\n",
        "If you have time and motivation left, you will find at the end of this notebook some additional work tracks.\n",
        "\n",
        "Of course, you are also free to look for others on your own, should you feel like it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVp8wO-DLPA6"
      },
      "source": [
        "### On neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-IlziLSLf8n"
      },
      "source": [
        "#### Basic facts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiqvXBKLjA2"
      },
      "source": [
        "An in-depth study of neural networks is not the object of this project, but it can be useful to have a rough idea of how they work. We will present them here briefly; the curious reader can consult the sources cited below, refer to the Deep Learning course of the curriculum or search by themslves, on the Internet for example.\n",
        "\n",
        "The perceptron, also called artificial neuron or formal neuron, tries to reproduce the function of a biological neuron. The objective of a neural network is to reproduce an arbitrarily complex function that associates an input $x$ with an output $y$. For example, we can create neural networks to recognize a dog from a cat: the input is then an image, and the output the word \"dog\" or \"cat\".\n",
        "\n",
        "A layer of a neural network is governed by the following equation:\n",
        "\n",
        "$$ \\hat{y} = f(\\mathbf{W} \\mathbf{X} + b) $$\n",
        "\n",
        "where\n",
        "- $\\mathbf{X}$ is the input matrix, usually consisting of several vectors $x_1, \\dots, x_m$\n",
        "- $\\hat{y}$ is the output vector\n",
        "- $\\mathbf{W}$ is a matrix of weights (parameters specific to the layer, which can be updated)\n",
        "- $b$ is a vector of weights called *bias* (also subject to evolution)\n",
        "- $f$ is a function called *activation function*, generally non-linear like the sigmoid, $\\mathrm{ReLU}$ or $\\tanh$ functions.\n",
        "\n",
        "![](https://user.oc-static.com/upload/2018/12/10/15444553183515_neuroneformel-1.png)\n",
        "\n",
        "The main thing to remember is that the layers of a neural network combine inputs $x_1, \\dots, x_m$ by linear (the $\\mathbf{W} \\mathbf{X} + b$) and non-linear ($f$) operations in the hope of obtaining an output $y$.\n",
        "\n",
        "In practice, at the initialization of the network, given an input $x$, we usually obtain an output $\\hat{y}$ which can be very different from $y$ : the parameters $\\mathbf{W}$ and $b$ are not yet appropriate.\n",
        "\n",
        "We then perform a *training* of the neural network which allows to reduce the distance between $\\hat{y}$ and $y$, i.e. to minimize the *loss function* $\\mathcal{L}(y, \\hat{y}) = \\| \\hat{y} - y \\|^2$. This is usually done by an optimization process such as gradient descent. We will skip the technical details (look at the sources for more information); just remember that by updating the parameters $\\mathbf{W}$ and $b$ incrementally, it is possible to improve the performance of the network as it will produce a $\\hat{y}$ similar to the desired $y$.\n",
        "\n",
        "![](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG.png)\n",
        "\n",
        "By stacking such layers, the network becomes more complex, hence, more elaborate mechanisms can be approximated. However, this usually requires a longer and more data-intensive training. The \"*Deep*\" in *Deep Learning* reflects the fact that we use multi-layered neural networks.\n",
        "\n",
        "As an illustration, here is a neural network that learns to separate crosses and circles: the more the training progresses (a step is commonly called *epoch*), the better it works.\n",
        "\n",
        "![](https://user.oc-static.com/upload/2018/12/12/15446484526497_linearsep_anim.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfQEqxtmLnuC"
      },
      "source": [
        "##### Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGY1hba_Lq5V"
      },
      "source": [
        "- [OpenClassrooms - Initiez-vous au Deep Learning](https://openclassrooms.com/fr/courses/5801891-initiez-vous-au-deep-learning)\n",
        "- [IBM - What are neural networks?](https://www.ibm.com/cloud/learn/neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Npnvxn8Ls5i"
      },
      "source": [
        "#### Recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPnMYkKbR41"
      },
      "source": [
        "Recurrent neural networks (RNN) are neural networks that are particularly adapted to process sequential data such as time series or text. Indeed, in a text, there are sequential dependencies between words; we cannot write a sentence in any order, in French or English at least.\n",
        "\n",
        "The difference between RNNs and classical neural networks (usually called *feed-forward neural networks*) is that the former have a \"memory\" represented by a hidden state that evolves along a \"time\" axis in any given layer.\n",
        "\n",
        "![](https://miro.medium.com/max/875/1*AQ52bwW55GsJt6HTxPDuMA.gif)\n",
        "![](https://miro.medium.com/max/875/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)\n",
        "![](https://miro.medium.com/max/875/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
        "\n",
        "There are many ways to implement such a memory; here we will use a type of RNN called LSTM (Long-Short Term Memory), which was the best performing model until the introduction of the *transformer* in 2017 (which *transformer* will not be on the agenda of this project; the curious reader may read the corresponding paper in the sources).\n",
        "\n",
        "![](https://miro.medium.com/max/875/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
        "\n",
        "The LSTM exhibits several interesting mechanisms; in particular, it has the ability to choose what it wishes to retain or forget in the long term. Refer to the sources for more detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm_4Cpy7L0Gf"
      },
      "source": [
        "##### Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsqeOTlwLwSY"
      },
      "source": [
        "- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [Towards data science - Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "- [Distill - Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)\n",
        "- [Transformer - Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRQm3DwlMMz-"
      },
      "source": [
        "## Data retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jedRZgXOerit"
      },
      "source": [
        "To begin with, we will have to retrieve a set of data relevant to our task.\n",
        "\n",
        "Since we want to generate alexandrines, we will have to retrieve texts in alexandrines. Luckily, a certain amount of French plays are available on  the [Dramacode](https://dramacode.github.io/) website.\n",
        "\n",
        "Copy the following code in the empty cell below and run it to mount your Google Drive on this notebook and download the dataset. Once it is done, delete the cell so that you do not run it again by mistake.\n",
        "\n",
        "```python\n",
        "!cd /content/drive/MyDrive/\"Colab Notebooks\" && wget -r -A '*.txt' dramacode.github.io/\n",
        "```\n",
        "\n",
        "You should find a `dramacode.github.io` folder in your `Colab Notebooks` folder. We will be using the `txt` files from the `naked` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYQvW8qcMmVd"
      },
      "outputs": [],
      "source": [
        "# copy the code here\n",
        "\n",
        "#!cd /content/drive/MyDrive/\"Colab Notebooks\" && wget -r -A '*.txt' dramacode.github.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmRK2CIRNYJ"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhoUv1WbnBX"
      },
      "source": [
        "### Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u-sQUPG1ggp"
      },
      "source": [
        "Now that we have retrieved our data, we will need to preprocess it. There are a few operations we might want to do:\n",
        "- Filter the files to keep only those which are in alexandrines\n",
        "- Remove the punctuation or add spaces around\n",
        "- Put every character to lower case\n",
        "- Stem the words\n",
        "- Lemmatize the words\n",
        "- Remove stop words\n",
        "\n",
        "Below we will create the functions for each of them. However, you are free to chose which ones you will use; for some, it is difficult to predict if they will improve the results. For others, there are good reasons to choose them, or not. Ideally, you could test every possible combination, but that would take a lot of time.\n",
        "\n",
        "Just keep in mind that we want to generate **natural** French language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bDjh1HcRLiN"
      },
      "source": [
        "### Text filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M93yoM0UJwia"
      },
      "source": [
        "We will only do some basic filtering based for instance on the length of the line.\n",
        "You could open `corneillep_cid.txt` and check the average length of the lines for example.\n",
        "\n",
        "\n",
        "The function **keep_alexandrines_only** takes an array of lines as input and returns a filtered array containing only the lines that are most likely to be alexandrines. It uses a regular expression to count the number of syllables in each line and removes the ones that don't have exactly 12 syllables.\n",
        "\n",
        "An alexandrine is a line of poetry that has 12 syllables, with a caesura (a pause) after the sixth syllable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3RvHDFJwA1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def keep_alexandrines_only(lines):\n",
        "    \"\"\"\n",
        "        Given an array of lines, returns the filtered array \n",
        "        containing lines which are the most likely to be alexandrines.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "\n",
        "    line_final = \"\"\n",
        "    for line in lines:\n",
        "      #line=line.lower().split()\n",
        "      p = re.compile(r\"(([aäâàeêéèëioôöuüûyEAUIOY]{1,3}(?:(?!e)[,\\s]*[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]))|([aäâàeéèêëioôöuüûyEAUIOY]{1,3}(?![e,\\s’.;])))\")#([aäâàeêéèëioôöuüûy]{1,3}(?:(?!e)[,\\s]*[b-df-hj-np-tv-z]))|([aäâàeéèêëioôöuüûy]{1,3}(?![e,\\s’.;]))\")\n",
        "      #syllables=0\n",
        "\n",
        "      x = p.findall(line)\n",
        "      syllables=len(x)\n",
        "      #print(syllables)\n",
        "      \n",
        "      if syllables != 12 :\n",
        "        lines.remove(line)\n",
        "\n",
        "    return lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Comments** \n",
        "\n",
        "The following code defines a Python function called test_keep_alexandrines_only that tests the keep_alexandrines_only function by printing out the first 50 lines of two different texts and highlighting which lines are alexandrines and which ones are not.\n",
        "\n",
        "The function first prints the header --- NOT ALEXANDRINES --- to indicate that the following lines are not alexandrines. It then opens a text file from the NAKED_PATH directory called allainval_ecoledesbourgeois.txt and reads the first 50 lines. The keep_alexandrines_only function is called on these lines to extract only the lines that are alexandrines. The resulting lines are printed out, with each line being preceded by the string [ALEXANDRINE] if it is an alexandrine, or [NOT ALEXANDRINE] if it is not.\n",
        "\n",
        "The function then prints the header --- ALEXANDRINES --- to indicate that the following lines are alexandrines. It opens a different text file from the NAKED_PATH directory called corneillep_cid.txt and reads the first 50 lines. The keep_alexandrines_only function is called again to extract only the lines that are alexandrines, and the resulting lines are printed out in the same way as before.\n",
        "\n",
        "This testing function is useful to ensure that the keep_alexandrines_only function correctly identifies alexandrines in a given text. By visually inspecting the output, one can verify that the function is correctly identifying the expected lines as alexandrines or not."
      ],
      "metadata": {
        "id": "6ZsNWb90wadM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ol0PAL0NJk5"
      },
      "outputs": [],
      "source": [
        "def test_keep_alexandrines_only():\n",
        "    \"\"\"ideally, the first section should have much fewer lines than the second part\"\"\"\n",
        "    print(\"--- NOT ALEXANDRINES ---\")\n",
        "    with open(os.path.join(NAKED_PATH, \"allainval_ecoledesbourgeois.txt\")) as fin:\n",
        "        test_lines = keep_alexandrines_only(fin.readlines()[:50])\n",
        "        \n",
        "        for line in test_lines:\n",
        "            \n",
        "            print(line)\n",
        "        \n",
        "    print(\"\\n--- ALEXANDRINES ---\")\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = keep_alexandrines_only(fin.readlines()[:50])\n",
        "        \n",
        "        for line in test_lines:\n",
        "            \n",
        "            print(line)\n",
        "        \n",
        "\n",
        "#test_keep_alexandrines_only()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWKQ95KoR8n"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwizD5uoLOl"
      },
      "source": [
        "```\n",
        "--- NOT ALEXANDRINES ---\n",
        "Est-ce que tu songes encore à Damis ?\n",
        "Madame, voilà Monsieur Mathieu qui vient d’entrer.\n",
        "Vous craignez qu’il ne goûte pas cette alliance ?\n",
        "Oh ! Mademoiselle n’en tient point.\n",
        "Il ne donnera jamais son consentement.\n",
        "\n",
        "--- ALEXANDRINES ---\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille, et brigue ma faveur,\n",
        "Don Rodrigue et Don Sanche à l’envi font paraître\n",
        "Le beau feu qu’en leurs coeurs ses beautés ont fait naître,\n",
        "Ce n’est pas que Chimène écoute leurs soupirs\n",
        "Ou d’un regard propice anime leurs désirs,\n",
        "Au contraire pour tout dedans l’indifférence\n",
        "Elle n’ôte à pas un, ni donne d’espérance,\n",
        "Et sans les voir d’un oeil trop sévère, ou trop doux,\n",
        "C’est de votre seul choix qu’elle attend un époux.\n",
        "Elle est dans le devoir, tous deux sont dignes d’elle,\n",
        "Tous deux formés d’un sang, noble, vaillants, fidèle,\n",
        "Jeunes, mais qui font lire aisément dans leurs yeux\n",
        "L’éclatante vertu de leurs braves aïeux.\n",
        "Don Rodrigue sur tout n’a trait en son visage\n",
        "Qui d’un homme de Cour ne soit la haute image,\n",
        "Et sort d’une maison si seconde en guerriers,\n",
        "Qu’ils y prennent naissance au milieu des lauriers,\n",
        "La valeur de son père, en son temps sans pareille,\n",
        "Tant qu’a duré sa force a passé pour merveille,\n",
        "Ses rides sur son front ont gravé ses exploits\n",
        "Et nous disent encor ce qu’il fut autrefois :\n",
        "Je me promets du fils ce que j’ai vu du père,\n",
        "Et ma fille en un mot peut l’aimer et me plaire.\n",
        "Va l’en entretenir, mais dans cet entretien\n",
        "Cache mon sentiment et découvre le sien,\n",
        "Je veux qu’à mon retour nous en parlions ensemble ;\n",
        "L’heure à présent m’appelle au conseil qui s’assemble,\n",
        "Le Roi doit à son fils choisir un gouverneur,\n",
        "Ou plutôt m’élever à ce haut rang d’honneur,\n",
        "Ce que pour lui mon bras chaque jour exécute,\n",
        "Me défend de penser qu’aucun me le dispute.\n",
        "Quelle douce nouvelle à ces jeunes amants !\n",
        "Et que tout se dispose à leurs contentements !\n",
        "Et bien, Elvire, enfin, que faut-il que j’espère ?\n",
        "Que dois-je devenir, et que t’as dit mon père ?\n",
        "Deux mots dont tous vos sens doivent être charmés,\n",
        "Il estime Rodrigue autant que vous l’aimez.\n",
        "L’excès de ce bonheur me met en défiance,\n",
        "Puisse à de tels discours donner quelque croyance ?\n",
        "Il passe bien plus outre, il approuve ses feux,\n",
        "Et vous doit commander de répondre à ses voeux.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvPRl_5BSF8g"
      },
      "source": [
        "As you can see, a few lines from the non-verse file sneaked through our rudimentary filter. To improve this, you could for instance compute the percentage of lines kept with respect to the original number of lines: if the ratio is too small, you could choose to keep nothing at all since it is likely that the file is not even in verse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oneyl4MpRQCn"
      },
      "source": [
        "### Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idg6UkZRhqvs"
      },
      "source": [
        "#### Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZq1yRxiRSG9"
      },
      "source": [
        "Here we will create functions to either remove punctuation or put spaces around it for better handling later. Let us write both, and you will choose which you want to do. (There is no good or bad answer)\n",
        "\n",
        "[Relevant documentation](https://docs.python.org/3/library/string.html#string.punctuation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsR1S4kvTutH"
      },
      "source": [
        "#### Punctuation spacing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSz2d815RmXi"
      },
      "outputs": [],
      "source": [
        "def space_punctuation(lines):\n",
        "    \"\"\"\n",
        "        given an array of lines, returns the lines with spaced punctuation.\n",
        "        Caution: inverted commas should ideally not be preceded by spaces.\n",
        "        For instance, \"j'ai faim\" should become \"j' ai faim\".\n",
        "        It is possible that the `’` symbol is used instead of `'` in the data.\n",
        "        The str.replace() and str.split() function might help.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    liste1 = [',',';',':','!','?','.','/',\"’\",\"<\",\">\"]\n",
        "    liste_totale = []\n",
        "    for line in lines :\n",
        "        for element in liste1:\n",
        "          if element in line :\n",
        "            if element == \"’\":\n",
        "              line = line.replace(element, element + ' ')\n",
        "            else :\n",
        "              line = line.replace(element, ' '+ element + ' ')\n",
        "        liste_totale.append(line)\n",
        "    return liste_totale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tZnK6SHT8CT"
      },
      "outputs": [],
      "source": [
        "def test_space_punctuation():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = space_punctuation(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "#test_space_punctuation()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuqGrC9xhn9H"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9dYFBYCoENh"
      },
      "source": [
        "```\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille , et brigue ma faveur ,\n",
        "Don Rodrigue et Don Sanche à l’ envi font paraître\n",
        "Le beau feu qu’ en leurs coeurs ses beautés ont fait naître ,\n",
        "Ce n’ est pas que Chimène écoute leurs soupirs\n",
        "Ou d’ un regard propice anime leurs désirs ,\n",
        "Au contraire pour tout dedans l’ indifférence\n",
        "Elle n’ ôte à pas un , ni donne d’ espérance ,\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ndOLkfWTxtr"
      },
      "source": [
        "#### Punctuation removal\n",
        "\n",
        "The **remove_punctuation** function takes an array of lines and returns the lines without punctuation, except for inverted commas which are kept along with a space after them. The function loops over each line in the input array and replaces each punctuation character with an empty string except for inverted commas which are replaced with a space after them. The modified lines are then stored in a new list which is returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW8Zpq6KV5dE"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(lines):\n",
        "    \"\"\"\n",
        "        given an array of lines, returns the lines without punctuation.\n",
        "        You might want to keep inverted commas and a space after them, though.\n",
        "        The str.replace() and str.split() function might help.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "\n",
        "    liste1 = [',',';',':','!','?','.','/',\"<\",\">\",\"\\n\"]\n",
        "    liste_totale = []\n",
        "    \n",
        "    for line in lines :\n",
        "      for element in liste1:\n",
        "        if element == \"’\":\n",
        "              line = line.replace(element, element + ' ')\n",
        "        else :\n",
        "              line = line.replace(element,'')\n",
        "      liste_totale.append(line)\n",
        "    return liste_totale\n",
        "      \n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Ywcd2oWKQ-"
      },
      "outputs": [],
      "source": [
        "def test_remove_punctuation():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = remove_punctuation(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "#test_remove_punctuation()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KgPg4lLhkqz"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_kkWSCpn-0t"
      },
      "source": [
        "```\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille et brigue ma faveur\n",
        "Don Rodrigue et Don Sanche à l’ envi font paraître\n",
        "Le beau feu qu’ en leurs coeurs ses beautés ont fait naître\n",
        "Ce n’ est pas que Chimène écoute leurs soupirs\n",
        "Ou d’ un regard propice anime leurs désirs\n",
        "Au contraire pour tout dedans l’ indifférence\n",
        "Elle n’ ôte à pas un ni donne d’ espérance\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMul6gQcXHTo"
      },
      "source": [
        "### Lower case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCd0AtMXKxg"
      },
      "source": [
        "This does not really need testing, since [a function already exists](https://docs.python.org/3/library/stdtypes.html?highlight=lower#str.lower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cnuePYSa1U"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4CJd84VSdEY"
      },
      "source": [
        "You might want to use the NLTK package. [Relevant documentation](https://www.nltk.org/api/nltk.stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgWtuC1L1axu",
        "outputId": "cf6f959b-9e68-48b4-fd08-44d0ad343663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 504 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.8.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **stem_lines** function uses remove_punctuation to clean up the input lines, then applies stemming to each word in the lines using the SnowballStemmer from NLTK, with a fallback to Spacy for words that aren't found in the stemmer's dictionary.\n",
        "\n",
        "It splits each line into words using .split(), applies stemming to each word using the SnowballStemmer (with a fallback to Spacy's lemma_ function), and then joins the stemmed words back together into a single line. The resulting list of stemmed lines is returned by the function.\n",
        "\n"
      ],
      "metadata": {
        "id": "jP24uj_kxPJE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtiMMu2aTRIk"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "stemmer = SnowballStemmer(language='french')\n",
        "\n",
        "def stemming(line):\n",
        "    doc = nlp(line)\n",
        "    dic=[stemmer.stem(X.text) for X in doc]\n",
        "    result=' '.join(dic)\n",
        "    return result\n",
        "\n",
        "def stem_lines(lines):\n",
        "    # TODO\n",
        "    #lines=remove_punctuation(lines)\n",
        "    liste_finale=[]\n",
        "    for line in lines :\n",
        "      line=stemming(line)\n",
        "      liste_finale.append(line)\n",
        "    return liste_finale  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYveNxzWT1Wd"
      },
      "outputs": [],
      "source": [
        "def test_stem_lines():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = stem_lines(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "#test_stem_lines()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni03DlR9hf7_"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqxLmR-n2rH"
      },
      "source": [
        "```\n",
        "entre tous ce amant dont la jeun ferveur\n",
        "ador votr fille, et brigu ma faveur,\n",
        "don rodrigu et don sanch à l’env font paraîtr\n",
        "le beau feu qu’en leur coeur se beaut ont fait naître,\n",
        "ce n’est pas que chimen écout leur soupir\n",
        "ou d’un regard propic anim leur désirs,\n",
        "au contrair pour tout dedan l’indifférent\n",
        "elle n’ôt à pas un, ni don d’espérance,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOJl9RBJiHa6"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Ih-rZHiI2C"
      },
      "source": [
        "You might want to use [SpaCy's lemmatizer](https://spacy.io/api/lemmatizer) here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGehQGXj7OMv",
        "outputId": "4d83c082-ad55-4334-9da7-893e1647b94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy-lefff\n",
            "  Downloading spacy-lefff-0.4.1.tar.gz (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-lefff) (3.4.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (0.8.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-lefff) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy-lefff) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->spacy-lefff) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-lefff) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-lefff) (2.0.1)\n",
            "Building wheels for collected packages: spacy-lefff\n",
            "  Building wheel for spacy-lefff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lefff: filename=spacy_lefff-0.4.1-py3-none-any.whl size=2929922 sha256=68457638d079ae30d6f44d0a67a78221e2bfd7403ee1d20f59093eb46dda421a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/54/72/5982052730ae8aea85b95eee7f1252a69413fcf33be05f175d\n",
            "Successfully built spacy-lefff\n",
            "Installing collected packages: spacy-lefff\n",
            "Successfully installed spacy-lefff-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy-lefff\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer\n",
        "from spacy.language import Language\n",
        "\n",
        "@Language.factory('french_lemmatizer')\n",
        "def create_french_lemmatizer(nlp, name):\n",
        "    return LefffLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze3Snxp7jS6g"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('fr_core_news_sm')\n",
        "nlp.add_pipe('french_lemmatizer', name='lefff')\n",
        "\n",
        "def lemmatizer(line):\n",
        "\n",
        "  nlp = spacy.load('fr_core_news_sm')\n",
        "  nlp.add_pipe('french_lemmatizer', name='lefff')\n",
        "  doc = nlp(line)\n",
        "\n",
        "  liste = []\n",
        "  for d in doc:\n",
        "    liste.append(d.lemma_)\n",
        "  result = ' '.join(element for element in liste )\n",
        "  return result\n",
        "\n",
        "def lemmatize_lines(lines):\n",
        "    new_lines=[]\n",
        "    for line in lines :\n",
        "      line=lemmatizer(line)\n",
        "      new_lines.append(line)\n",
        "    return(new_lines)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmhLV5Rlk6bN"
      },
      "outputs": [],
      "source": [
        "def test_lemmatize_lines():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = lemmatize_lines(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "#test_lemmatize_lines()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ABc1YTWhTT1"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwJXlZJ_ntuy"
      },
      "source": [
        "```\n",
        "entre tout ce amant dont le jeune ferveur\n",
        "Adore votre fille , et brigue mon faveur ,\n",
        "Don Rodrigue et Don sanche à l’ envi faire paraître\n",
        "le beau feu qu’ en leur coeur son beauté avoir faire naître ,\n",
        "ce n’ être pas que chimène écout leur soupir\n",
        "ou d’ un regard propice anime leur désir ,\n",
        "au contraire pour tout dedans l’ indifférence\n",
        "lui n’ ôte à pas un , ni donne d’ espérance ,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OtXBdZfXjPj"
      },
      "source": [
        "### Combining the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWiFBYmxmzWL"
      },
      "source": [
        "We have several preprocessing functions at our disposal. Now, you can choose which ones you want to use to create the final corpus.\n",
        "\n",
        "Again, remember that we want to generate **natural** French language."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function **create_corpus** creates a corpus of text by iterating through a directory of text files, selecting only certain files, and then processing each file to extract lines that contain alexandrines. The function then applies several processing steps on these lines, such as space_punctuation, remove_punctuation, and writing them to a text file named corpus.txt. Finally, the function returns the list of processed lines."
      ],
      "metadata": {
        "id": "isMm54lnxwtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_uWAJZXnye"
      },
      "outputs": [],
      "source": [
        "def create_corpus():\n",
        "    # Folder Path\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/dramacode.github.io/naked/\" \n",
        "      \n",
        "    # Change the directory\n",
        "    os.chdir(path)\n",
        " \n",
        "    data = []\n",
        "    liste = [\"corneillep_cid.txt\", \"corneillep_veuve.txt\"]\n",
        "    \n",
        "    # iterate through all file\n",
        "    for file in os.listdir(path) : #NAKED_PATH:\n",
        "        \n",
        "        # Check whether file is in text format or not\n",
        "        if file in liste:\n",
        "            print(file)\n",
        "            file_path = file\n",
        "\n",
        "            with open(os.path.join(NAKED_PATH,file_path )) as fin:\n",
        "                lines=fin.readlines()[2:150]\n",
        "                lines1=keep_alexandrines_only(lines)\n",
        "                lines2=space_punctuation(lines1)\n",
        "                lines3=remove_punctuation(lines2)\n",
        "                for line in lines3:\n",
        "                    print(line)\n",
        "                    f = open(os.path.join(path, 'corpus.txt'), \"w\")\n",
        "                    f.write(line)\n",
        "                    f.write(\"\\n\")\n",
        "                    f.close()\n",
        "                    data.append(line)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWXOObOZeRHc"
      },
      "source": [
        "You should obtain a file `corpus.txt` which is about 700,000 lines long (but it might be noticeably more or less depending on your filtering choices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IA19Ebm-93v"
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwr84mMi_AKB"
      },
      "source": [
        "Now we will split the data in three datasets: `train`, `valid` and `test`. The `train` dataset will be used to train the model, ie, update its parameters. The `valid` dataset will be used to monitor the evolution of the loss during training, and the `test` dataset will be used to evaluate the model after training. We could do without the `test` dataset here since we will be looking at the generated output ourselves, but this is a common procedure so let us follow it.\n",
        "\n",
        "For instance, every 100 lines could be split as 80 / 10 / 10 or 90 / 5 / 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4CeOWeq_TRI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Folder Path\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/dramacode_corpus/\" #\"\"Enter Folder Path\"\n",
        "  \n",
        "# Change the directory\n",
        "os.chdir(path)\n",
        "data_liste=[\"train_df\" , \"val_df\" , \"test_df\"]\n",
        "file_liste=[\"train.txt\" , \"valid.txt\" , \"test.txt\"]\n",
        "\n",
        "\n",
        "def split_corpus(data_liste,file_liste):\n",
        "    with open(os.path.join(CORPUS_PATH,\"corpus.txt\" )) as f:\n",
        "      corpus = np.random.permutation(f.readlines())\n",
        "      n = len(corpus)\n",
        "      n_train = int(n*0.8)\n",
        "      n_valid = int(n*0.1)\n",
        "      n_test = int(n*0.1)\n",
        "      train = pd.DataFrame(corpus[:n_train])\n",
        "      valid = pd.DataFrame(corpus[n_train:n_train+n_valid])\n",
        "      test = pd.DataFrame(corpus[n_train+n_valid:n_train+n_valid+n_test])\n",
        "\n",
        "    with open(os.path.join(CORPUS_PATH,\"train3.txt\"), 'a') as file: \n",
        "      np.savetxt(file, train.values, fmt='%s' , delimiter='', newline='')\n",
        "    \n",
        "    with open(os.path.join(CORPUS_PATH,\"test3.txt\"), 'a') as file:\n",
        "      np.savetxt(file, test.values, fmt='%s' , delimiter='', newline='')\n",
        "\n",
        "    with open(os.path.join(CORPUS_PATH,\"valid3.txt\"), 'a') as file:\n",
        "      np.savetxt(file, valid.values, fmt='%s', delimiter='', newline='')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-NKMi19B2Jb",
        "outputId": "80f82220-a374-422f-c60e-b1b6712b85af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47358\n"
          ]
        }
      ],
      "source": [
        "#split_corpus(data_liste, file_liste)\n",
        "with open(os.path.join(CORPUS_PATH,\"valid3.txt\" )) as f:\n",
        "  lines = f.readlines()\n",
        "  print(len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCKFi6rS94Wk"
      },
      "source": [
        "## Implementing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csNjTtrXfpVi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnfqi6Q99qB"
      },
      "source": [
        "We will be using the [PyTorch](https://pytorch.org/docs/stable/index.html) framework. For a detailed explanation about PyTorch, please refer to their [Learn the Basics tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-7Ll_syEPZQ"
      },
      "outputs": [],
      "source": [
        "import os, time, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pprint  # for pretty printing\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n55a8GU_Nt-Q",
        "outputId": "0d4d6f77-3bbc-4276-fc3f-39c37b035d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# use CUDA if on a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  # should be `cpu` for now, we will change runtime later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An6qm3I0GBkT"
      },
      "source": [
        "### Building the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYbzSbLQGF3b"
      },
      "source": [
        "First, we need to build a dictionary so that we can pass words as unique identifiers, usually integers, to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC7vyFpqGE4v"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\" \n",
        "        This function should check if the word is in word2idx; if it\n",
        "        is not, it should add it with the first available index\n",
        "        \"\"\"\n",
        "        if word not in self.word2idx:\n",
        "          i = self.__len__()\n",
        "          self.word2idx[word] = i\n",
        "          self.idx2word[i] = word\n",
        "\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djfrHUstHk4C"
      },
      "source": [
        "### Building the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Corpus class** is used to create a text corpus. \n",
        "\n",
        "It has an __init__ method that initializes a dictionary, which is a collection of unique words, and three lists of tokenized text data: train, valid, and test. \n",
        "\n",
        "The class also has a **tokenize** method that takes a path to a text file as input, reads the text file, and tokenizes the text into a list of integers corresponding to the indices of the words in the dictionary. \n",
        "\n",
        "The tokenize method also adds new words to the dictionary if they do not already exist."
      ],
      "metadata": {
        "id": "7fcohcwsx8jH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj_ACXMxHqPA"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train3.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid3.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test3.txt'))\n",
        "        self.eos_token = '<eos>'  # end of sentence (line) token\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"\n",
        "            Tokenizes a text file, knowing the dictionary, in order to \n",
        "            tranform it into a list of indices.\n",
        "            The str.split() function might be useful.\n",
        "        \"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        \n",
        "        with open(os.path.join(path), 'r') as fin:\n",
        "          for line in fin.readlines():\n",
        "            #if line != \"\\n\" :\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "              self.dictionary.add_word(word)\n",
        "        self.dictionary.add_word('<eos>')\n",
        "        \n",
        "        # Once done, effectively tokenize by adding the tokens to a vector.\n",
        "        # We want the `ids` vector to be an int64 torch tensor containing all \n",
        "        # tokens in the order of the file.\n",
        "        # Lines should all end with the sos token.\n",
        "        \n",
        "        # TODO\n",
        "        ids_ = []\n",
        "        with open(os.path.join(path), 'r') as fin:\n",
        "          for line in fin.readlines():\n",
        "            #if line != \"\\n\" :\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "              id = self.dictionary.word2idx[word]\n",
        "              ids_.append(id)\n",
        "            id = self.dictionary.word2idx[\"<eos>\"]\n",
        "            ids_.append(id)\n",
        "          ids = torch.tensor(ids_, dtype=torch.int64)\n",
        "\n",
        "        return ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoECuQBCOlBh"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnQoQejCOj8t"
      },
      "outputs": [],
      "source": [
        "corpus = None  # TODO, create the corpus\n",
        "\n",
        "path=CORPUS_PATH\n",
        "corpus = Corpus(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOhjA_fl3QiN",
        "outputId": "a9823d22-9b7b-4b67-ce22-79eb502f5ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3583080])\n",
            "torch.Size([448461])\n",
            "torch.Size([447787])\n"
          ]
        }
      ],
      "source": [
        "print(corpus.train.shape)\n",
        "print(corpus.test.shape)\n",
        "print(corpus.valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knuYPoKlPgdP"
      },
      "source": [
        "### Batching the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQvUUfMdf5Kf"
      },
      "source": [
        "#### `batchify`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y56wY9aiPi4w"
      },
      "source": [
        "Our `corpus.train`, `corpus.valid` and `corpus.test` tensors are flat tensors; they cannot efficiently be processed with a gpu.\n",
        "We will therefore want to change their shape so that they are more square-like.\n",
        "\n",
        "With the alphabet being our data, we currently have the sequence:\n",
        "`[a b c d e f g h i j k l m n o p q r s t u v w x y z]`. We want to reorganize it as independant batches that will be processed independently by the model !\n",
        "\n",
        "For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences (represented as columns):\n",
        "```\n",
        "┌ a g m s ┐\n",
        "│ b h n t │\n",
        "│ c i o u │\n",
        "│ d j p v │\n",
        "│ e k q w │\n",
        "└ f l r x ┘\n",
        "```\n",
        "with the last two elements being lost.\n",
        "\n",
        "Again, these columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "❓ **Question**: what do you think of this batching process? For instance, is it adapted to our data? If yes, how so? If not, how could we improve it?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **batchify** function takes a tensor data and a batch size bsz and returns a new tensor that contains data split into bsz equal-sized chunks. \n",
        "\n",
        "The function first calculates the remainder when dividing the length of data by bsz. If this remainder is not zero, the function discards the remaining result number of elements in data. \n",
        "\n",
        "The function then calculates the size of each batch (n_size) as len(data)//bsz. \n",
        "\n",
        "Finally, the function uses the torch.reshape() function to reshape data to a size of (bsz, n_size), transposes this tensor to obtain a tensor of size (n_size, bsz) and returns this new tensor. \n",
        "\n",
        "The tensor is also made contiguous and moved to the appropriate device."
      ],
      "metadata": {
        "id": "exCglrYlyNlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikqKozdCQUZi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def batchify(data, bsz):\n",
        "    \"\"\"        \n",
        "        Three steps:\n",
        "        1. Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        2. Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "        3. Evenly divide the data across the bsz batches.\n",
        "        Note: You might need to use `.contiguous()` at the end because PyTorch can be somewhat strict about memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO\n",
        "    result= (len(data)) % bsz\n",
        "    n_size=len(data)//bsz\n",
        "\n",
        "    if result != 0 :\n",
        "      data=data[:-result]\n",
        "      batch_size = len(data)//bsz\n",
        "    else : \n",
        "      n_size = len(data)//bsz\n",
        "    data= torch.reshape(data,(bsz,n_size) ).transpose(1,0)\n",
        "\n",
        "    return data.contiguous().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfFr-ev9SKv9",
        "outputId": "0a1a6eb5-832c-4cec-b611-d12012655972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  6, 12, 18],\n",
            "        [ 1,  7, 13, 19],\n",
            "        [ 2,  8, 14, 20],\n",
            "        [ 3,  9, 15, 21],\n",
            "        [ 4, 10, 16, 22],\n",
            "        [ 5, 11, 17, 23]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def test_batchify():\n",
        "    data = torch.tensor(np.arange(26))\n",
        "    print(batchify(data, 4))\n",
        "\n",
        "test_batchify()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAfYXoJsk_iB"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bmaIOdfag4"
      },
      "source": [
        "\n",
        "```\n",
        "tensor([[ 0,  6, 12, 18],\n",
        "        [ 1,  7, 13, 19],\n",
        "        [ 2,  8, 14, 20],\n",
        "        [ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22],\n",
        "        [ 5, 11, 17, 23]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqgElWQf-oN"
      },
      "source": [
        "#### `get_batch` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ErZUp5ceHAs"
      },
      "source": [
        "Now we will build a function `get_batch` which subdivides the source data into chunks of the appropriate length. This function will be used later in the model training and evaluation.\n",
        "\n",
        "If source is equal to the example output of the batchify function, with\n",
        "a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "\n",
        "```\n",
        "┌ a g m s ┐ ┌ b h n t ┐\n",
        "| b h n t | | c i o u │\n",
        "└ c i o u ┘ └ d j p v ┘\n",
        "```\n",
        "\n",
        "The first variable contains the letters input to the network, while the second\n",
        "contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "\n",
        "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the `batchify` function. The chunks are along dimension 0, corresponding to the `seq_len` dimension in the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM_hcoPaeEpK"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i, seq_len):\n",
        "    \"\"\"\n",
        "        Should return (data, target) where data would be the first variable of \n",
        "        the example above, and target the second variable.\n",
        "\n",
        "        - source is the source data;\n",
        "        - i is the position of the current sequence;\n",
        "        - seq_len is the sequence length;\n",
        "\n",
        "        Three steps:\n",
        "        1. Deal with the possibility that there's not enough data left for a full sequence\n",
        "        2. Take the input data\n",
        "        3. Shift by one for the target data\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO\n",
        "    rest = (len(source.flatten())) % seq_len\n",
        "    data = source [i : seq_len+i]\n",
        "\n",
        "    target = source [i+1: seq_len+i+1]\n",
        "    if len(target) < len(data) :\n",
        "      data=data[ : len(target)]\n",
        "      \n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWzCPKSegdEF",
        "outputId": "7a6a7ada-682b-44db-9e65-2a5649638602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[ 2,  8, 14, 20],\n",
            "        [ 3,  9, 15, 21],\n",
            "        [ 4, 10, 16, 22]], device='cuda:0'),\n",
            " tensor([[ 3,  9, 15, 21],\n",
            "        [ 4, 10, 16, 22],\n",
            "        [ 5, 11, 17, 23]], device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "def test_get_batch():\n",
        "    source = torch.tensor(np.arange(26))\n",
        "    source = batchify(source, 4)\n",
        "    pp.pprint(get_batch(source, 2, 3))\n",
        "\n",
        "test_get_batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVOaoTI9lTZ4"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu6SpjVBlU7d"
      },
      "source": [
        "```\n",
        "(tensor([[ 2,  8, 14, 20],\n",
        "        [ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22]]),\n",
        " tensor([[ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22],\n",
        "        [ 5, 11, 17, 23]]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEBXfAYWgHeQ"
      },
      "source": [
        "#### Batchifying the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZlA4vbATaNh"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128  # you can choose other values\n",
        "EVAL_BATCH_SIZE = 16  # you can choose other values\n",
        "\n",
        "train_data = batchify(corpus.train , BATCH_SIZE) #None\n",
        "val_data = batchify(corpus.valid , EVAL_BATCH_SIZE) \n",
        "test_data = batchify(corpus.test , EVAL_BATCH_SIZE) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtAA00OrT9bA",
        "outputId": "ce295140-9dee-4589-eaa5-c077a8bd000c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3583080]) torch.Size([27992, 128])\n",
            "torch.Size([447787]) torch.Size([27986, 16])\n",
            "torch.Size([448461]) torch.Size([28028, 16])\n"
          ]
        }
      ],
      "source": [
        "print(corpus.train.shape, train_data.shape)\n",
        "print(corpus.valid.shape, val_data.shape)\n",
        "print(corpus.test.shape, test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXnac_6vlqeQ"
      },
      "source": [
        "##### Expected result (your own corpus lengths and batch sizes may be different)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1DAP2E5lyWK"
      },
      "source": [
        "```\n",
        "torch.Size([6987667]) torch.Size([54591, 128])\n",
        "torch.Size([874006]) torch.Size([54625, 16])\n",
        "torch.Size([872488]) torch.Size([54530, 16])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxxbwZxWM3v"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfSjvjtlWl4N"
      },
      "source": [
        "Models are usually implemented as custom nn.Module subclasses.\n",
        "- We need to redefine the __init__ method, which creates the object.\n",
        "- We also need to redefine the forward method, which transform the input into outputs.\n",
        "- We can also add any method that we need: here, in order to initiate weights in the model.\n",
        "\n",
        "Relevant documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM).\n",
        "\n",
        "Below is the model we will use; since you are not very familiar with how neural networks work yet, it is already filled and you can use it as a black box. However, you may want to look at it and try to understand how it works. Feel free to discuss about it together; if needed, you can also ask me questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vro0grY6WKUB"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2, initrange=0.1):\n",
        "        \"\"\"\n",
        "            ntoken: length of the dictionary,\n",
        "            ninp: dimension of the input,\n",
        "            nhid: dimension of the hidden layers,\n",
        "            nlayers: number of layers,\n",
        "            dropout: regularization parameter\n",
        "            initrange: range for weight initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.initrange = initrange\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-recurrent weights \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution,\n",
        "        # between -self.initrange and +self.initrange, and the decoder bias with zeros\n",
        "        # https://pytorch.org/docs/stable/nn.init.html?highlight=init\n",
        "        # - the methods uniform_() and zeros_() might help\n",
        "        # - self.encoder has a .weight attribute\n",
        "        # - self.decoder has .weight and .bias attributes\n",
        "\n",
        "        nn.init.uniform_(self.encoder.weight, -self.initrange, self.initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -self.initrange, self.initrange)\n",
        "        \n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "\n",
        "    def forward(self, input, hidden1):\n",
        "\n",
        "        # Process the input with the encoder, then dropout\n",
        "        emb = self.drop(self.encoder(input))\n",
        "\n",
        "        # Apply the LSTMs\n",
        "        output, hidden2 = self.rnn(emb, hidden1)\n",
        "        \n",
        "        # Decode into scores\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "\n",
        "        return decoded, hidden2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJbvgKxU9jY"
      },
      "source": [
        "❓ **Question**: how does the method `LSTMModel.init_hidden` work?\n",
        "\n",
        "## **Réponse :**\n",
        "\n",
        "**La méthode permet en instanciant la classe d'initier un modèle LSTM, à partir de la taille du dictionnaire, des dimensions de l'input, d'une couche cachée, du nombre de couches cachées et la limite pour les valeurs initiales de poids à l'intérieur du modèle** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m83eDABcUE8"
      },
      "source": [
        "## Running the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabjdocxSppI"
      },
      "source": [
        "In this section, everything is already implemented. However, there are a few questions below that you should try to answer so as to understand what is going on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-V77T9cZP3"
      },
      "source": [
        "### Initializing the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQCAADxqaIud",
        "outputId": "6f871ab3-35be-4cb3-80a0-e70158856952"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f44d82f9d90>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 42  # you can choose other values\n",
        "torch.manual_seed(SEED) # set the random seed manually for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYjCCAam0psk"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 512  # you can choose other values\n",
        "HIDDEN_SIZE = 1024  # you can choose other values\n",
        "N_LAYERS = 2  # you can choose other values\n",
        "DROPOUT = 0.2  # you can choose other values\n",
        "criterion = nn.CrossEntropyLoss()  # maps the output of a Linear layer to a probability distribution                                   # see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "N_TOKENS = len(corpus.dictionary)  # length of the dictionary\n",
        "model = LSTMModel(N_TOKENS, EMBEDDING_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT).to(device)  # initialize the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mybRNEeXQX1"
      },
      "outputs": [],
      "source": [
        "# You do not need to thoroughly understand the contents of this cell.\n",
        "# However, some resources are available in the comments if you are curious.\n",
        "# The values are not necessarily optimal, by the way;\n",
        "# you could try to tune them later if you have the time.\n",
        "\n",
        "LR = 10 # https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10\n",
        "OPTIMIZER = 'sgd'  # https://ruder.io/optimizing-gradient-descent/\n",
        "WDECAY = 0  # https://d2l.ai/chapter_multilayer-perceptrons/weight-decay.html\n",
        "CLIP = 0.25  # https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem\n",
        "if OPTIMIZER == 'sgd':\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "elif OPTIMIZER == 'adam':\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "else:\n",
        "    optim = None\n",
        "\n",
        "if OPTIMIZER in ['sgd', 'adam']:\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.7)\n",
        "else:\n",
        "    scheduler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7yI-GVbdF-o"
      },
      "outputs": [],
      "source": [
        "# Other global parameters\n",
        "EPOCHS = 10  # number of rounds of training, you can choose other values\n",
        "SEQ_LEN = 30  # length of the sequences, you can choose other values\n",
        "LOG_INTERVAL = 100  # for logging purposes, you can choose other values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxhr9sULycPz"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ySLy7eLyY2m"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFD9d9_nAC9"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    \n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LEN)):\n",
        "        data, targets = get_batch(train_data, i, SEQ_LEN)\n",
        "\n",
        "        if optim is not None:\n",
        "            optim.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output.view(-1, N_TOKENS), targets.view(-1))\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if CLIP is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "        if optim is None:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "        else:\n",
        "            optim.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            if optim is None:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // SEQ_LEN, lr,\n",
        "                    elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            else:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // SEQ_LEN, scheduler.get_last_lr()[-1],\n",
        "                    elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTmS1aIGyeRA"
      },
      "outputs": [],
      "source": [
        "def evaluate(source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(EVAL_BATCH_SIZE)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, source.size(0) - 1, SEQ_LEN):\n",
        "            data, targets = get_batch(source, i, SEQ_LEN)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, N_TOKENS), targets.view(-1)).item()\n",
        "    return total_loss / (len(source) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGmq3kUzaaQO"
      },
      "outputs": [],
      "source": [
        "liste=[\";\",\".\",\"!\",\":\",\"?\",\"»\"]\n",
        "\n",
        "def generate(source, n_words, temperature=1, topk=0):\n",
        "    \"\"\"\n",
        "        n_words: number of words to generate\n",
        "        fout: optional output file\n",
        "    \"\"\"\n",
        "    vocab_to_int = corpus.dictionary.word2idx\n",
        "    int_to_vocab = corpus.dictionary.idx2word\n",
        "\n",
        "    model.eval()\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    source = source.split()\n",
        "    hidden = model.init_hidden(1)\n",
        "    for v in hidden:\n",
        "        v = v.to(device)\n",
        "    for w in source:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, hidden = model(ix, hidden)\n",
        "    output = output / temperature\n",
        "    if topk > 0:\n",
        "        probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "        indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "        idx_max = np.random.choice(indices, 1, p=probas)[0]\n",
        "    else:\n",
        "        idx_max = int(torch.argmax(softmax(output[0])).cpu().detach().numpy())\n",
        "    words = []\n",
        "    words.append(int_to_vocab[idx_max])\n",
        "    for i in range(1, n_words):\n",
        "        ix = torch.tensor([[idx_max]]).to(device)\n",
        "        output, hidden = model(ix, hidden)\n",
        "        output = output / temperature\n",
        "        if topk > 0:\n",
        "            probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "            indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "            idx_max = np.random.choice(indices, 1, p=probas)[0]\n",
        "        else:\n",
        "            idx_max = int(torch.argmax(softmax(output[0])).cpu().detach().numpy())\n",
        "        words.append(int_to_vocab[idx_max])\n",
        "    text = \" \".join(words)\n",
        "    for punc in liste :\n",
        "      if punc in text :\n",
        "        text = text.replace(punc, \"\")\n",
        "    text = text.replace(\"<eos>\", \"\\n\")\n",
        "\n",
        "    pp.pprint(text)\n",
        "    #return pp.pprint(text) #words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiOwJgcNs4BS"
      },
      "source": [
        "**Questions** whose answers will help you better understand what is happening:\n",
        "1. What are `model.train()` and `model.eval()` for?\n",
        "2. What is the `hidden` variable in the `train()` and `evaluate()` functions?\n",
        "3. What is `loss.backward()` in `train()` for?\n",
        "4. What is `optim.step()` for?\n",
        "5. What is `repackage_hidden()` for?\n",
        "6. How does the `generate()` function work?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Réponses :**\n",
        "\n",
        "- model.train() permet d'entrainer le modèle sur le corpus train.\n",
        "- model.eval() permet d'évaluer les performances du modèles sur les données de test.\n",
        "- La variable hidden dans train() et evaluate() permet d'initialiser un état caché (hidden state) pour effectuer de nouvel prédictions.\n",
        "- loss.backward() permet de mettre à jour les paramètres qui ont été spécifiées grace à la méthode requires_grad=True, à partir du gradient à chaque epoch de l'entrainement.\n",
        "- La fonction generate permet pour une séquence de mots donnée, de prédire une distribution de probabilités à partir du modèle pour le dictionnaire. Différentes méthodes ici (top k. et grredy Sampling entre autres) sont ensuite appliquées au résultat pour choisir le mot le plus probable; et ainsi de suite.  "
      ],
      "metadata": {
        "id": "AkETAWg6bMEN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bb_sSBfvKrz"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDvRxyozSppL"
      },
      "source": [
        "⚠ Now is a good time to switch to a GPU runtime. You might need to run all cells again; make sure that the output of the cell with the `device` variable is `cuda`. ⚠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsYR1wZyvlJM",
        "outputId": "db8de87f-761b-41bd-9c1a-f33cd92011f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded model from /content/drive/MyDrive/Colab Notebooks/dramacode_checkpoints/model4.pt\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'model4'\n",
        "SAVE_PATH = os.path.join(CHECKPOINT_PATH, '{}.pt'.format(MODEL_NAME))\n",
        "LOAD_EXISTING_MODEL = True  # Change to False if you want to start from scratch\n",
        "                            # ⚠ If False, any existing model with the same name\n",
        "                            # will be overwritten ⚠\n",
        "\n",
        "if LOAD_EXISTING_MODEL:\n",
        "    try:\n",
        "        with open(SAVE_PATH, 'rb') as f:\n",
        "            model = torch.load(f)\n",
        "            # after load the rnn params are not a continuous chunk of memory\n",
        "            # this makes them a continuous chunk, and will speed up forward pass\n",
        "            model.rnn.flatten_parameters()\n",
        "        print(\"Successfully loaded model from {}\".format(SAVE_PATH))\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "best_val_loss = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm3GKtvzRYOy",
        "outputId": "872b5587-00f3-4f1a-f5c7-d9de1f8edee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Nov 27 07:40:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W /  70W |   2294MiB / 15109MiB |     14%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCc0XAKRzJHX"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_PATH, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB9ULQAq2qZD",
        "outputId": "7216a735-ff1c-400c-ba54-803f044876d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.71 | test ppl   110.99\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyGu-GRo5Odu",
        "outputId": "41d18234-53d3-4650-b0be-7b1a422f193e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Il devait \n",
            "('vous faire un peu de puissance \\n'\n",
            " ' Et que de mon amour je ne puis vous défendre \\n'\n",
            " ' Et que de mon amour je ne puis vous défendre \\n'\n",
            " ' Et')\n"
          ]
        }
      ],
      "source": [
        "# Generate some text with Greedy Search\n",
        "\n",
        "# TODO\n",
        "\n",
        "sources = [\" Il devait \"]#,\"Je suis\" , \"Il ne sera\",\" Jamais au grand jamais\"]\n",
        "\n",
        "for source in sources :\n",
        "  print(source)\n",
        "  generate(source, 30, temperature=1, topk=0)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Ergm-Ms1Zx",
        "outputId": "d8bcab0d-08db-44c7-9d11-f018f48ec5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dans ces aimables\n",
            "('\\n'\n",
            " ' Et que de ce grand coeur vous pouvez faire entendre  \\n'\n",
            " ' Je le sais mais pour moi de ce que je vous dois    \\n'\n",
            " ' Je vous ai dit Seigneur et je vous en promets \\n'\n",
            " ' Que vous ne vous trompez de me voir davantage \\n'\n",
            " ' Je le vois mais pour moi de vous en faire grâce \\n'\n",
            " ' Je vous laisse en effet ce qui me doit entendre  \\n'\n",
            " ' Que je suis malheureux et je')\n",
            "\n",
            "\n",
            "Je suis\n",
            "('trop à vous de vous voir en ce lieu  \\n'\n",
            " ' Que je suis malheureux  Et que vous ne voulez \\n'\n",
            " ' Que je suis en ces mots la gloire de la guerre \\n'\n",
            " ' Et que pour vous punir il est temps de la voir \\n'\n",
            " ' Et si pour vous servir je ne puis vous défendre \\n'\n",
            " ' Que vous ne me devez pas vous en dire assez \\n'\n",
            " ' Je le veux bien encor pour le moins je le veux \\n')\n",
            "\n",
            "\n",
            "Il ne sera\n",
            "('point de plus de puissance    \\n'\n",
            " ' Que de vous voir encor le soin de votre époux    Et que vous me verrez je '\n",
            " 'vous en fais la cause  \\n'\n",
            " ' Et si je puis à moi le sort qui me dévore  \\n'\n",
            " \" Et si vous ne pensez à la gloire d'un autre \\n\"\n",
            " ' Que vous ne pouvez rien me faire une injure  \\n'\n",
            " ' Je vous le rends un bien mais je vous suis')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate some text with Top K Search\n",
        "\n",
        "sources = [\"Dans ces aimables\",\"Je suis\" , \"Il ne sera\"]\n",
        "\n",
        "for source in sources :\n",
        "  print(source)\n",
        "  generate(source, 80, temperature=1, topk=3)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Beam Search :**\n",
        "\n",
        "The \"beam_search_decoder\" function performs beam search decoding on a given input sequence using a pre-trained language model.\n",
        "\n",
        "The function takes the following arguments:\n",
        "\n",
        "-- **source**: A string representing the input sequence.\n",
        "\n",
        "-- **k**: An integer representing the beam width, i.e., the number of candidates to consider at each decoding step.\n",
        "\n",
        "-- **n_words**: An integer representing the maximum length of the output sequence.\n",
        "-- **temperature** : A float representing the softmax temperature used to control the randomness of the output.\n",
        "\n",
        "- The function first initializes the vocabulary of the language model using a dictionary that maps each word to an integer index. \n",
        "- It then converts the input sequence into a list of words and feeds it into the language model to obtain the hidden states of the model.\n",
        "\n",
        "- The function then uses a beam search algorithm to generate the output sequence : \n",
        "  - the algorithm maintains a set of candidate sequences and at each decoding step, \n",
        "  - it generates k new candidates by extending each of the current candidates with all possible next words according to the language model probabilities. \n",
        "  - The candidates are then ranked according to a score that combines the language model probabilities and the length of the sequence, and \n",
        "  - the k candidates with the highest score are selected to form the new set of candidate sequences.\n",
        "\n",
        "The beam search decoding continues until the maximum length n_words is reached, or until all candidate sequences end with a special end-of-sentence token.\n",
        "\n",
        "Finally, the function selects the highest-scoring candidate sequence and converts it back into a string representation. It removes any punctuation characters from the output sequence and replaces any remaining end-of-sentence tokens with a newline character before printing the output using the pprint function.\n"
      ],
      "metadata": {
        "id": "H-FqW8RHptFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AGJAR16NKzl"
      },
      "outputs": [],
      "source": [
        "## Generate with Beam search\n",
        "liste=[\";\",\".\",\"!\",\":\",\"?\",\"»\"]\n",
        "\n",
        "def beam_search_decoder(source, k, n_words, temperature=5):\n",
        "\n",
        "  vocab_to_int = corpus.dictionary.word2idx\n",
        "  int_to_vocab = corpus.dictionary.idx2word\n",
        "  words = []\n",
        "\n",
        "  model.eval()\n",
        "  softmax = nn.Softmax(dim=-1)\n",
        "  source = source.split()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for v in hidden:\n",
        "      v = v.to(device)\n",
        "  for w in source:\n",
        "      ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "      output, hidden = model(ix, hidden)\n",
        "  output = output / temperature\n",
        "\n",
        "  row  = softmax(output[0]).cpu().detach().numpy()[0]\n",
        "  l = list()\n",
        "  \n",
        "  sequences = [[l, 0.]]\n",
        "  all_candidates = list()\n",
        "  n = 0\n",
        "\n",
        "  for i in range(len(sequences)):\n",
        "    seq, score = sequences[i]\n",
        "    for j in range(len(row)) :\n",
        "      candidate = [seq + [j], (score - np.log(row[j]))/len(int_to_vocab)] \n",
        "      all_candidates.append(candidate)\n",
        "  # order all candidates by score\n",
        "\n",
        "  ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "  sequences = ordered[:k]\n",
        "\n",
        "# Changement commence ici !!!\n",
        "  for sequence in sequences:\n",
        "        indice = sequence[0]\n",
        "        ix = torch.tensor([[indice]]).to(device)\n",
        "        output, hidden = model(ix, hidden) \n",
        "        output = output / temperature\n",
        "        row = softmax(output[0]).cpu().detach().numpy()[0]\n",
        "        l = list()\n",
        "\n",
        "        all_candidates = list()\n",
        "\n",
        "        # expand each current candidate\n",
        "        for j in range(len(row)) :\n",
        "            if j not in index :\n",
        "              candidate = [seq + [j], (score - np.log(row[j]))/len(int_to_vocab)] \n",
        "              all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        sequences = ordered[:k]\n",
        "        n += 1\n",
        "  \n",
        "  # loop through potential candidates\n",
        "  \n",
        "  while n < n_words :\n",
        "    for sequence in sequences:\n",
        "        indice = sequence[0]\n",
        "        ix = torch.tensor([[indice[-1]]]).to(device)\n",
        "        output, hidden = model(ix, hidden) \n",
        "        output = output / temperature\n",
        "        row = softmax(output[0]).cpu().detach().numpy()[0]\n",
        "        l = list()\n",
        "\n",
        "        all_candidates = list()\n",
        "\n",
        "        # expand each current candidate\n",
        "        for j in range(len(row)) :\n",
        "            if j not in index :\n",
        "              candidate = [seq + [j], (score - np.log(row[j]))/len(int_to_vocab)] \n",
        "              all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        sequences = ordered[:k]\n",
        "        n += 1\n",
        "\n",
        "  index = sequences[0][0]       \n",
        "\n",
        "  for element in index :\n",
        "      w = int_to_vocab[element]\n",
        "      words.append(w)\n",
        "  text = source\n",
        "  text = \" \".join(words)\n",
        "  for punc in liste :\n",
        "    if punc in text :\n",
        "      text = text.replace(punc, \"\")\n",
        "  text = text.replace(\"<eos>\", \"\\n\")\n",
        "\n",
        "  pp.pprint(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Examples of generation with 30 words**"
      ],
      "metadata": {
        "id": "BEtunyDOq_Kh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sJuJel_OanF",
        "outputId": "519ee294-76a3-474e-f6bb-e28657429c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Je suis\n",
            "('bien de vous voir que je ne puis croire \\n'\n",
            " \" Et si ce n'est pas là le plus grand des humains    Je suis trop heureux et \"\n",
            " 'mon')\n",
            "\n",
            "\n",
            "Je suis\n",
            "('bien de vous voir que je ne puis croire \\n'\n",
            " \" Et si ce n'est pas là le plus grand des humains    Je suis trop heureux et \"\n",
            " 'mon')\n",
            "\n",
            "\n",
            "Je suis\n",
            "('bien de vous voir que je ne puis croire \\n'\n",
            " \" Et si ce n'est pas là le plus grand des humains    Je suis trop heureux et \"\n",
            " 'mon')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# generate_beam(\"Je suis\", 10, temperature=1, n_beam=3)\n",
        "\n",
        "sources = [\"Je suis\" ]\n",
        "\n",
        "for source in sources :\n",
        "  print(source)\n",
        "  beam_search_decoder(source, 5, 30, temperature = 1)\n",
        "print(\"\\n\")\n",
        "\n",
        "for source in sources :\n",
        "  print(source)\n",
        "  beam_search_decoder(source, 3, 30, temperature = 1)\n",
        "print(\"\\n\")\n",
        "\n",
        "for source in sources :\n",
        "  print(source)\n",
        "  beam_search_decoder(source, 50, 30, temperature = 1)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Top P Sampling with Low threshold :**\n",
        "\n",
        "The top_p_decoder function generates text using the top-p sampling strategy with a low probability threshold value.\n",
        "\n",
        "The function takes the following arguments:\n",
        "\n",
        "-- **source**: A string representing the input sequence.\n",
        "\n",
        "-- **s**: A float representing the probability threshold value for the top-p sampling strategy.\n",
        "\n",
        "-- **n_words**: An integer representing the maximum length of the output sequence.\n",
        "\n",
        "-- **temperature**: A float representing the softmax temperature used to control the randomness of the output.\n",
        "\n",
        "- The function first initializes the vocabulary of the language model using a dictionary that maps each word to an integer index. \n",
        "\n",
        "- It then converts the input sequence into a list of words and feeds it into the language model to obtain the hidden states of the model.\n",
        "\n",
        "The function then performs top-p sampling to generate the output sequence. \n",
        "\n",
        "- At each decoding step, the function :\n",
        "  - computes the softmax probabilities of the next words according to the language model, \n",
        "  - sorts the probabilities in descending order, and \n",
        "  - selects the subset of words that have a cumulative probability lower than the probability threshold value s. \n",
        "\n",
        "- The function then randomly samples one of the remaining words from this subset, and uses it as the next word in the output sequence.\n",
        "\n",
        "The top-p sampling continues until the maximum length n_words is reached, or until a special end-of-sentence token is generated.\n",
        "\n",
        "Finally, the function selects the generated sequence and converts it back into a string representation. It removes any punctuation characters from the output sequence and replaces any remaining end-of-sentence tokens with a newline character before printing the output using the pprint function."
      ],
      "metadata": {
        "id": "1bZjbWKXrIYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNHp_VW-_JCa"
      },
      "outputs": [],
      "source": [
        "## Generate with Top_p with low threshold value\n",
        "liste = [\";\",\".\",\"!\",\":\",\"?\",\"»\"]\n",
        "\n",
        "\n",
        "def top_p_decoder(source, s, n_words, temperature=1.):\n",
        "\n",
        "  # sampler = NucleusSampler(0.95, TemperatureSampler(temperature)) #NucleusSampler(s,Sampler)\n",
        "\n",
        "  vocab_to_int = corpus.dictionary.word2idx\n",
        "  int_to_vocab = corpus.dictionary.idx2word\n",
        "  words=[]\n",
        "\n",
        "  model.eval()\n",
        "  softmax = nn.Softmax(dim=-1)\n",
        "  source = source.split()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for v in hidden:\n",
        "      v = v.to(device)\n",
        "  for w in source :\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = model(ix, hidden) \n",
        "    output = output / temperature\n",
        "\n",
        "  row = softmax(output[0])[0].clone().detach()\n",
        "  \n",
        "  indices = torch.argsort(row, dim=0, descending=True)\n",
        "  tri =[]\n",
        "  for indice in indices :\n",
        "    tri.append(float(row[indice].cpu().detach()))\n",
        "  cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "  l_tmp = torch.vstack([indices,cum_probs])\n",
        "  \n",
        "  l = torch.tensor([cum_prob <= s for cum_prob in cum_probs])\n",
        "  n_idx = torch.sum(l) + 1\n",
        "\n",
        "  index_tmp = []\n",
        "\n",
        "  for e in (l_tmp[0][:n_idx]) :\n",
        "    index_tmp.append(int(e))\n",
        "  ix = np.random.choice(index_tmp)\n",
        "  \n",
        "  index = []\n",
        "  index.append(ix)\n",
        "  n = len(index)\n",
        "\n",
        "  while n < n_words :\n",
        "\n",
        "    output, hidden = model(torch.tensor([[ix]]).to(device), hidden) \n",
        "    output = output / temperature\n",
        "    row = softmax(output[0])[0].clone().detach()\n",
        "\n",
        "    ## Récupération des indices de probas les plus élevés\n",
        "\n",
        "    indices = torch.argsort(row, dim=0, descending=True).to(device)\n",
        "    \n",
        "    tri =[]\n",
        "\n",
        "    for indice in indices :\n",
        "      tri.append(float(row[indice].cpu().detach()))\n",
        "    cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "    l_tmp = torch.vstack([indices,cum_probs])\n",
        "    \n",
        "    l = torch.tensor([cum_prob <= s for cum_prob in cum_probs])\n",
        "    n_idx = torch.sum(l) + 1\n",
        "\n",
        "    index_tmp = []\n",
        "\n",
        "    for e in (l_tmp[0][:n_idx]) :\n",
        "      index_tmp.append(int(e))\n",
        "    ix = np.random.choice(index_tmp)\n",
        "    index.append(ix)\n",
        "    ix = torch.tensor([[ix]]).to(device)\n",
        "    n = len(index)      \n",
        "\n",
        "  for element in index :\n",
        "      w = int_to_vocab[element]\n",
        "      words.append(w)\n",
        "      \n",
        "  text = source\n",
        "  text = \" \".join(words)\n",
        "  for punc in liste :\n",
        "    if punc in text :\n",
        "      text = text.replace(punc, \"\")\n",
        "  text = text.replace(\"<eos>\", \"\\n\")\n",
        "\n",
        "  pp.pprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Examples of generation of 20 words and 0.05 of threshold:**"
      ],
      "metadata": {
        "id": "IRDKlJVsr0pA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee8SFBcZGlIe",
        "outputId": "a79ff370-be1a-4bf2-d184-54f40783d6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('ne puis vous voir que pour vous en instruire \\n'\n",
            " ' Et que de mon amour je ne puis vous défendre')\n"
          ]
        }
      ],
      "source": [
        "source = 'Je'\n",
        "'''(suis bien de vous voir que je vous le jure \\n'\n",
        " ' Et que de mon amour je ne puis vous')'''\n",
        "\n",
        "liste_temperature =[0.1 , 0.5 , 1. , 2 , 5]\n",
        "\n",
        "#for v in liste_temperature :\n",
        "top_p_decoder(source, 0.05, 20, temperature=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Top P Sampling with High threshold :**\n",
        "\n",
        "The top_p_decoder_desc function generates text using the top-p sampling strategy with a high probability threshold value.\n",
        "\n",
        "The function takes the following arguments:\n",
        "\n",
        "-- **source**: A string representing the input sequence.\n",
        "\n",
        "-- **s**: A float representing the probability threshold value for the top-p sampling strategy.\n",
        "\n",
        "-- **n_words** : An integer representing the maximum length of the output sequence.\n",
        "\n",
        "-- **temperature**: A float representing the softmax temperature used to control the randomness of the output.\n",
        "\n",
        "- The function first initializes the vocabulary of the language model using a dictionary that maps each word to an integer index. \n",
        "\n",
        "- It then converts the input sequence into a list of words and feeds it into the language model to obtain the hidden states of the model.\n",
        "\n",
        "The function then performs top-p sampling to generate the output sequence. \n",
        "\n",
        "- At each decoding step, the function : \n",
        "  - computes the softmax probabilities of the next words according to the language model, \n",
        "  - sorts the probabilities in ascending order, and \n",
        "  - selects the subset of words that have a cumulative probability greater than or equal to the probability threshold value s. \n",
        "  - The function then randomly samples one of the remaining words from this subset, and \n",
        "  - uses it as the next word in the output sequence.\n",
        "\n",
        "The top-p sampling continues until the maximum length n_words is reached, or until a special end-of-sentence token is generated.\n",
        "\n",
        "Finally, the function selects the generated sequence and converts it back into a string representation. It removes any punctuation characters from the output sequence and replaces any remaining end-of-sentence tokens with a newline character before printing the output using the pprint function."
      ],
      "metadata": {
        "id": "EnokW6HmsJxN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6jtpngD74s3"
      },
      "outputs": [],
      "source": [
        "## Generate with Top_p with high threshold value\n",
        "\n",
        "liste = [\";\",\".\",\"!\",\":\",\"?\",\"»\"]\n",
        "\n",
        "def top_p_decoder_desc(source, s, n_words, temperature=1.):\n",
        "\n",
        "  vocab_to_int = corpus.dictionary.word2idx\n",
        "  int_to_vocab = corpus.dictionary.idx2word\n",
        "  words=[]\n",
        "\n",
        "  model.eval()\n",
        "  softmax = nn.Softmax(dim=-1)\n",
        "  source = source.split()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for v in hidden:\n",
        "      v = v.to(device)\n",
        "  for w in source :\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = model(ix, hidden) \n",
        "    output = output / temperature\n",
        "\n",
        "  row = softmax(output[0])[0].clone().detach()\n",
        "  \n",
        "  indices = torch.argsort(row, dim=0, descending=False) \n",
        "  tri =[]\n",
        "\n",
        "  for indice in indices :\n",
        "    tri.append(float(row[indice].cpu().detach()))    \n",
        "  cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "  l_tmp = torch.vstack([indices,cum_probs])\n",
        "  l = torch.tensor([cum_prob >= s for cum_prob in cum_probs])\n",
        "  n_idx = torch.sum(l) -1\n",
        "\n",
        "  index_tmp = []\n",
        "  x = len(l_tmp[0]) - n_idx - 1\n",
        "  for e in (l_tmp[0][x:]) :\n",
        "    index_tmp.append(int(e))\n",
        "\n",
        "  ix = np.random.choice(index_tmp)\n",
        "  index = []\n",
        "  index.append(ix)\n",
        "  n = len(index)\n",
        "\n",
        "  while n < n_words :\n",
        "    temperature = 0.5\n",
        "    output, hidden = model(torch.tensor([[ix]]).to(device), hidden)\n",
        "    row = softmax(output[0])[0].clone().detach()\n",
        "\n",
        "    indices = torch.argsort(row, dim=0, descending=False).to(device)\n",
        "    tri = []\n",
        "\n",
        "    for indice in indices :\n",
        "      tri.append(float(row[indice].cpu().detach()))\n",
        "    cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "    l_tmp = torch.vstack([indices,cum_probs])\n",
        "\n",
        "    l = torch.tensor([cum_prob >= s for cum_prob in cum_probs])\n",
        "    n_idx = torch.sum(l) - 1\n",
        "\n",
        "    index_tmp = []\n",
        "    x = len(l_tmp[0]) - n_idx -1\n",
        "    for e in (l_tmp[0][x:]) :\n",
        "      index_tmp.append(int(e))\n",
        "\n",
        "    ix= np.random.choice(index_tmp)\n",
        "\n",
        "    index.append(ix)\n",
        "    n = len(index)      \n",
        "\n",
        "  for element in index :\n",
        "      w = int_to_vocab[element]\n",
        "      words.append(w)\n",
        "  text=source\n",
        "  text = \" \".join(words)\n",
        "  for punc in liste :\n",
        "    if punc in text :\n",
        "      text = text.replace(punc, \"\")\n",
        "  text = text.replace(\"<eos>\", \"\\n\")\n",
        "\n",
        "  pp.pprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Examples of generation of 20 words and 0.90 of threshold:**"
      ],
      "metadata": {
        "id": "1k2nv9N6tL8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpeTSkTGDEFy",
        "outputId": "aa0570d7-9575-43eb-d4b9-74c6dfa20d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('veux vous dire et je ne puis comprendre \\n'\n",
            " ' Et je ne puis souffrir que de vous en instruire \\n')\n"
          ]
        }
      ],
      "source": [
        "source = 'Je'\n",
        "'''(suis bien de vous voir que je vous le jure \\n'\n",
        " ' Et que de mon amour je ne puis vous')'''\n",
        "\n",
        "liste_temperature =[ 1. ]\n",
        "\n",
        "for v in liste_temperature :\n",
        "  top_p_decoder_desc(source, 0.90, 20, temperature=v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Combining Top P Sampling with High threshold and Top k approaches :**\n",
        "\n",
        "\n",
        "The top_p_k_decoder function generates text using a combination of top-p and top-k sampling strategies.\n",
        "\n",
        "The function takes the following arguments:\n",
        "\n",
        "-- **source**: A string representing the input sequence.\n",
        "\n",
        "-- **s**: A float representing the probability threshold value for the top-p sampling strategy.\n",
        "\n",
        "-- **n_words**: An integer representing the maximum length of the output sequence.\n",
        "\n",
        "-- **temperature**: A float representing the softmax temperature used to control the randomness of the output.\n",
        "\n",
        "- The function first initializes the vocabulary of the language model using a dictionary that maps each word to an integer index.\n",
        "\n",
        "- It then converts the input sequence into a list of words and feeds it into the language model to obtain the hidden states of the model.\n",
        "\n",
        "The function then performs top-p and top-k sampling to generate the output sequence. \n",
        "\n",
        "- At each decoding step, the function : \n",
        "  - computes the softmax probabilities of the next words according to the language model, \n",
        "  - selects the top-k words with the highest probabilities, and \n",
        "  - computes the subset of words that have a cumulative probability greater than or equal to the probability threshold value **s**. \n",
        "  - The function then randomly samples one of the remaining words from this subset, and \n",
        "  - uses it as the next word in the output sequence.\n",
        "\n",
        "The top-p and top-k sampling continue until the maximum length n_words is reached, or until a special end-of-sentence token is generated.\n",
        "\n",
        "Finally, the function selects the generated sequence and converts it back into a string representation. It removes any punctuation characters from the output sequence and replaces any remaining end-of-sentence tokens with a newline character before printing the output using the pprint function."
      ],
      "metadata": {
        "id": "NIoLmT4NtrUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6rtFktaERk_"
      },
      "outputs": [],
      "source": [
        "def top_p_k_decoder(source, s, n_words, temperature=1.):\n",
        "\n",
        "  vocab_to_int = corpus.dictionary.word2idx\n",
        "  int_to_vocab = corpus.dictionary.idx2word\n",
        "  words=[]\n",
        "\n",
        "  model.eval()\n",
        "  softmax = nn.Softmax(dim=-1)\n",
        "  source = source.split()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for v in hidden:\n",
        "      v = v.to(device)\n",
        "  for w in source :\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = model(ix, hidden) \n",
        "    output = output / temperature\n",
        "\n",
        "  row = softmax(output[0])[0].clone().detach()\n",
        "  \n",
        "  indices = torch.argsort(row, dim=0, descending=False) \n",
        "  tri =[]\n",
        "\n",
        "  for indice in indices :\n",
        "    tri.append(float(row[indice].cpu().detach()))\n",
        "  cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "  l_tmp = torch.vstack([indices,cum_probs])\n",
        "  l = torch.tensor([cum_prob >= s for cum_prob in cum_probs])\n",
        "\n",
        "  topk = torch.sum(l)\n",
        "  probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "  indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "  ix = np.random.choice(indices, 1, p=probas)[0]\n",
        "\n",
        "  index = []\n",
        "  index.append(ix)\n",
        "  n = len(index)\n",
        "\n",
        "  while n < n_words :\n",
        "    output, hidden = model(torch.tensor([[ix]]).to(device), hidden)\n",
        "    row = softmax(output[0])[0].clone().detach()\n",
        "\n",
        "    indices = torch.argsort(row, dim=0, descending=False).to(device)\n",
        "    tri =[]\n",
        "\n",
        "    for indice in indices :\n",
        "      tri.append(float(row[indice].cpu().detach()))\n",
        "    cum_probs = torch.cumsum(torch.tensor(tri).to(device), dim=-1)\n",
        "    l_tmp = torch.vstack([indices,cum_probs])\n",
        "\n",
        "    l = torch.tensor([cum_prob >= s for cum_prob in cum_probs])\n",
        "\n",
        "    topk = torch.sum(l)\n",
        "    probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "    indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "    ix = np.random.choice(indices, 1, p=probas)[0]\n",
        "\n",
        "    index.append(ix)\n",
        "    n = len(index)      \n",
        "\n",
        "  for element in index :\n",
        "      w = int_to_vocab[element]\n",
        "      words.append(w)\n",
        "  text=source\n",
        "  text = \" \".join(words)\n",
        "  for punc in liste :\n",
        "    if punc in text :\n",
        "      text = text.replace(punc, \"\")\n",
        "  text = text.replace(\"<eos>\", \"\\n\")\n",
        "\n",
        "  pp.pprint(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Examples of generation of 50 words and 0.90 of threshold:**"
      ],
      "metadata": {
        "id": "ITiTU9httXcl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwDiODY8GsgU",
        "outputId": "fc0bec3f-4f92-47d2-ea5d-be85b21e38f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('ne puis me résoudre à ce que je puis \\n'\n",
            " ' Je ne puis plus souffrir que de vous en donner \\n'\n",
            " ' Et je ne puis souffrir que de votre personne \\n'\n",
            " ' Et que pour vous servir il faut que je vous quitte \\n'\n",
            " ' Et que de ce grand jour je')\n"
          ]
        }
      ],
      "source": [
        "source ='Je'\n",
        "top_p_k_decoder(source, 0.90, 50, temperature=1.)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}